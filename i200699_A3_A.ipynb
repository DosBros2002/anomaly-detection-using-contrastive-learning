{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FlROpNLbh7-a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n",
        "test_labels = pd.read_csv('/content/test_label.csv')\n",
        "\n",
        "# Preprocessing data: Normalize and convert to tensors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(train_data)\n",
        "X_test = scaler.transform(test_data)\n",
        "y_test = test_labels.values  # Assuming labels are in the first column\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers=1, num_heads=2, dim_feedforward=128):\n",
        "        super(TransformerAutoencoder, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, input_dim)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.decoder = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)  # Project input data to the expected dimension\n",
        "        x = x.unsqueeze(1)  # Add a dummy batch dimension [batch_size, 1, feature_size]\n",
        "        encoded = self.transformer_encoder(x)\n",
        "        decoded = self.decoder(encoded.squeeze(1))\n",
        "        return decoded\n",
        "\n",
        "autoencoder = TransformerAutoencoder(input_dim=train_data.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-yfxlPBkfqI",
        "outputId": "c1ca839c-8f9f-41f8-cc2d-a43d0770ea35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(train_data.shape[1], 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "discriminator = Discriminator()\n"
      ],
      "metadata": {
        "id": "gkrF36IskdKt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(features, batch_size, temperature=0.05):\n",
        "    labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0)\n",
        "    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
        "    features = nn.functional.normalize(features, dim=1)\n",
        "\n",
        "    similarity_matrix = torch.matmul(features, features.T)\n",
        "    positives = similarity_matrix[range(batch_size), range(batch_size)]\n",
        "    negatives = similarity_matrix[~labels.bool()].reshape(batch_size, -1)\n",
        "\n",
        "    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
        "    labels = torch.zeros(batch_size, dtype=torch.long)\n",
        "\n",
        "    return nn.CrossEntropyLoss()(logits / temperature, labels)\n"
      ],
      "metadata": {
        "id": "dcA_WLdAkytg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_gen = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
        "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for data in train_loader:\n",
        "        optimizer_disc.zero_grad()\n",
        "        real_data = data[0]\n",
        "        real_data = real_data.unsqueeze(1)  # Ensure the input has the right shape\n",
        "        fake_data = autoencoder(real_data).detach()\n",
        "        real_loss = nn.BCELoss()(discriminator(real_data), torch.ones(real_data.size(0), 1))\n",
        "        fake_loss = nn.BCELoss()(discriminator(fake_data), torch.zeros(fake_data.size(0), 1))\n",
        "        disc_loss = (real_loss + fake_loss) / 2\n",
        "        disc_loss.backward()\n",
        "        optimizer_disc.step()\n",
        "\n",
        "        # Train generator (autoencoder)\n",
        "        optimizer_gen.zero_grad()\n",
        "        reconstructed_data = autoencoder(real_data)\n",
        "        gen_loss = nn.MSELoss()(reconstructed_data, real_data)\n",
        "        gen_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss D: {disc_loss.item()}, Loss G: {gen_loss.item()}')\n",
        "\n",
        "# Add testing and evaluation as needed\n"
      ],
      "metadata": {
        "id": "uhpGc7hqk05s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}